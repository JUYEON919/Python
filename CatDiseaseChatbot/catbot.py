#!/usr/bin/env python
# coding: utf-8

# ê³ ì–‘ì´ ì§ˆë³‘ ìê°€ì§„ë‹¨ ì±—ë´‡

# In[2]:


import gradio as gr
import pandas as pd
from langchain_community.chat_models import ChatOllama
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain, LLMChain
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from fuzzywuzzy import process
import re


# In[3]:


# ê³ ì–‘ì´ ì§ˆë³‘ ê´€ë ¨ ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” CSV íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
df = pd.read_csv("./data/cat_diseases.csv", encoding='CP949')


# In[4]:


# Symptomsì™€ Description ì»¬ëŸ¼ì„ í•©ì³ì„œ ìƒˆë¡œìš´ inputs ì»¬ëŸ¼ ìƒì„±
# ì´ ì»¬ëŸ¼ì€ ê²€ìƒ‰ì— ì‚¬ìš©ë  í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.
df['inputs'] = df['Symptoms'].fillna('') + " " + df['Description'].fillna('')


# In[5]:


# í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¼ì • í¬ê¸°ë¡œ ë¶„í• í•©ë‹ˆë‹¤ (chunk_size: 500, chunk_overlap: 200)
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=200)
texts = text_splitter.split_text("\n".join(df['inputs']))


# In[6]:


# í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìœ„í•´ HuggingFaceì˜ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")


# In[7]:


# ë¶„í• ëœ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
vectorstore = FAISS.from_texts(texts, embeddings)


# In[8]:


# ChatOllama ëª¨ë¸ ì´ˆê¸°í™” (ì˜¨ë„ê°’ temperatureëŠ” 0.0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¼ê´€ëœ ì‘ë‹µ ìƒì„±)
try:
    llm = ChatOllama(model="gemma2", temperature=0.0)
except Exception as e:
    print(f"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    llm = None


# In[9]:


# ì§ˆë¬¸ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
question_generator_template = """
ì´ì „ ëŒ€í™” ë‚´ì—­ê³¼ ìƒˆë¡œìš´ ì‚¬ìš©ì ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ê²€ìƒ‰ì„ ìœ„í•œ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¿”ì„œ ìƒì„±í•´ì£¼ì„¸ìš”.

ì´ì „ ëŒ€í™” ë‚´ì—­:
{chat_history}

ìƒˆë¡œìš´ ì‚¬ìš©ì ì§ˆë¬¸:
{question}

ë…ë¦½ì ì¸ ì§ˆë¬¸:
"""
QUESTION_GENERATOR_PROMPT = PromptTemplate(input_variables=["chat_history", "question"], template=question_generator_template)


# In[10]:


# ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (í•œêµ­ì–´ì— ìµœì í™”)
combine_documents_template = """
ë‹¹ì‹ ì€ ê²½í—˜ ë§ì€ ê³ ì–‘ì´ ìˆ˜ì˜ì‚¬ì…ë‹ˆë‹¤. **ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ì—¬, ì–´ìƒ‰í•¨ì´ ì—†ë„ë¡ ë§¤ìš° ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì—¬ ì •ì¤‘í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.** ì™¸êµ­ì–´ë‚˜ ì–´ìƒ‰í•œ í•œêµ­ì–´ í‘œí˜„ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì‹­ì‹œì˜¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.

1.  ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€ì„ **í•œêµ­ì–´ë¡œ** ì œê³µí•©ë‹ˆë‹¤.
2.  í•„ìš”í•œ ê²½ìš°, ì¶”ê°€ì ì¸ ì§ˆë¬¸ì„ í†µí•´ ìƒí™©ì„ ëª…í™•íˆ íŒŒì•…í•˜ë ¤ê³  ë…¸ë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¦ìƒì˜ ê¸°ê°„, ì‹¬ê°ì„±, ë‹¤ë¥¸ ë™ë°˜ ì¦ìƒ ë“±ì„ **í•œêµ­ì–´ë¡œ, ë¶€ë“œëŸ½ê²Œ** ë¬¼ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜ˆ: "í˜¹ì‹œ ì–¸ì œë¶€í„° ê·¸ëŸ¬ì…¨ë‚˜ìš”?", "ë‹¤ë¥¸ ë¶ˆí¸í•œ ì ì€ ì—†ìœ¼ì‹ ê°€ìš”?")
3.  ê°€ëŠ¥í•œ ì›ì¸ ì§ˆë³‘ì„ ì–¸ê¸‰í•˜ê³ , ê° ì§ˆë³‘ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…ì„ **í•œêµ­ì–´ë¡œ, ì´í•´í•˜ê¸° ì‰½ê²Œ** ì œê³µí•©ë‹ˆë‹¤.
4.  ì§‘ì—ì„œ í•  ìˆ˜ ìˆëŠ” ì¡°ì¹˜ì™€ ë™ë¬¼ë³‘ì› ë°©ë¬¸ì´ í•„ìš”í•œ ê²½ìš°ë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ **í•œêµ­ì–´ë¡œ, ì¹œì ˆí•˜ê²Œ** ì•ˆë‚´í•©ë‹ˆë‹¤.
5.  ì ˆëŒ€ ì§„ë‹¨ì´ë‚˜ ì²˜ë°©ì„ ë‚´ë¦¬ì§€ ì•Šê³ , ë°˜ë“œì‹œ ë™ë¬¼ë³‘ì›ì— ë°©ë¬¸í•˜ì—¬ ì •í™•í•œ ì§„ë£Œë¥¼ ë°›ì„ ê²ƒì„ **í•œêµ­ì–´ë¡œ, ì •ì¤‘í•˜ê²Œ** ê¶Œì¥í•´ì•¼ í•©ë‹ˆë‹¤.
6.  ì‚¬ìš©ìê°€ ê±±ì •í•˜ê±°ë‚˜ ë¶ˆì•ˆí•œ ê°ì •ì„ ëŠë‚„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, **ìœ„ë¡œ**ì™€ **ì•ˆì‹¬**ì„ ì£¼ëŠ” ë§ì„ í¬í•¨í•˜ì—¬ ì‚¬ìš©ìê°€ ë” í¸ì•ˆí•˜ê²Œ ëŠë¼ë„ë¡ í•©ë‹ˆë‹¤.

ê²€ìƒ‰ëœ ë¬¸ì„œ:
{context}

ì‚¬ìš©ì ì§ˆë¬¸:
{question}

ë‹µë³€:
"""
COMBINE_DOCUMENTS_PROMPT = PromptTemplate(input_variables=["context", "question"], template=combine_documents_template)


# In[11]:


# Conversational Retrieval Chain ìƒì„±
# ì´ì „ ëŒ€í™” ë‚´ì—­ê³¼ ì‚¬ìš©ì ì§ˆë¬¸ì„ ê²°í•©í•´ ë…ë¦½ì ì¸ ì§ˆë¬¸ ìƒì„± í›„ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
question_generator = LLMChain(llm=llm, prompt=QUESTION_GENERATOR_PROMPT)
doc_chain = load_qa_chain(llm, chain_type="stuff", prompt=COMBINE_DOCUMENTS_PROMPT)

qa = ConversationalRetrievalChain(
    retriever=vectorstore.as_retriever(search_kwargs={"k": 1}),  # ê²€ìƒ‰ëœ ìƒìœ„ 1ê°œì˜ ë¬¸ì„œ ì‚¬ìš©
    combine_docs_chain=doc_chain,
    question_generator=question_generator,
    return_source_documents=True,
)

chat_history = []  # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬


# In[12]:


# ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜ (íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜)
def clean_text(text):
    text = text.strip()
    text = re.sub(r'[^\w\s]', '', text)
    text = text.replace(" ", "")
    return text.lower()


# In[13]:


# ê³ ì–‘ì´ ì§ˆë³‘ ì •ë³´ ê²€ìƒ‰ í•¨ìˆ˜
def search_disease_info(message):
    return "ì§ˆë³‘ ì •ë³´ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤..."


# In[14]:


# ê³ ì–‘ì´ ì§ˆë³‘ì— ëŒ€í•œ ì •ë³´ ì œê³µí•˜ëŠ” ì±„íŒ… í•¨ìˆ˜
def chat(message, history):
    cleaned_message = clean_text(message)
    disease_info = search_disease_info(message)
    return disease_info


# In[15]:


# Conversational Chat í•¨ìˆ˜ êµ¬í˜„ (chat_history ê´€ë¦¬)
def conversational_chat(message, history):
    # QA ìˆ˜í–‰
    result = qa({"question": message, "chat_history": history})
    
    # Gradioê°€ historyë¥¼ ê´€ë¦¬í•˜ë¯€ë¡œ historyë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ
    response = result["answer"]
    return response  # ì‘ë‹µë§Œ ë°˜í™˜


# ì¦ìƒ,ë‚˜ì´ ì…ë ¥í•˜ëŠ” ê°„ë‹¨ ì§„ë‹¨

# In[16]:


# ì„¸ ë²ˆì§¸ íƒ­ìš© í•¨ìˆ˜
custom_prompt = """
ë‹¹ì‹ ì€ ê³ ì–‘ì´ ì§ˆë³‘ì— ëŒ€í•´ ì „ë¬¸ì ì¸ ìˆ˜ì˜ì‚¬ ì—­í• ì„ í•©ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ê±±ì •í•˜ê±°ë‚˜ ë¶ˆì•ˆí•œ ê°ì •ì„ ëŠë‚„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, **ìœ„ë¡œ**ì™€ **ì•ˆì‹¬**ì„ ì£¼ëŠ” ë§ì„ í¬í•¨í•˜ì—¬ ì‚¬ìš©ìê°€ ë” í¸ì•ˆí•˜ê²Œ ëŠë¼ë„ë¡ í•©ë‹ˆë‹¤.
ì•„ë˜ ì¦ìƒì— ê¸°ë°˜í•˜ì—¬ ê°€ëŠ¥í•œ ì§ˆë³‘ê³¼ ê´€ë ¨ ì •ë³´ë¥¼ ì‘ì„±í•˜ì„¸ìš”:

ì¦ìƒ: {symptom}

ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
PROMPT_TEMPLATE = PromptTemplate(input_variables=["symptom"], template=custom_prompt)
chain = LLMChain(llm=llm, prompt=PROMPT_TEMPLATE)


# In[17]:


# ì‚¬ìš©ìê°€ ì–‘ì‹ì— ì…ë ¥í•œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜
def predict_from_form(symptom, age, duration):
    prompt = f"""
    ê³ ì–‘ì´ ë‚˜ì´: {age}
    ì¦ìƒ ë°œìƒ ê¸°ê°„: {duration}ì¼
    ì¦ìƒ: {symptom}
    
    ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ëŠ¥í•œ ì§ˆë³‘ê³¼ ëŒ€ì²˜ ë°©ë²•ì„ ì‘ì„±í•˜ì„¸ìš”.
    """
    response = chain.run(symptom=prompt)
    return response


# csvë°ì´í„°ê¸°ë°˜ ë°”ë¡œ ì‘ë‹µ ê°€ëŠ¥í•œ ì§„ì§œ ê°„ë‹¨ ì§„ë‹¨

# In[18]:


# ë“œë¡­ë‹¤ìš´ì—ì„œ ì„ íƒí•œ ì¦ìƒì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë³‘ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜
def predict_disease_from_card(symptom):
    # Fuzzy Matchingì„ ì‚¬ìš©í•´ ì¦ìƒê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ ì°¾ìŒ
    best_match = process.extractOne(symptom, df['Symptoms'].dropna())
    if best_match and best_match[1] > 80:  # ìœ ì‚¬ë„ê°€ 80% ì´ìƒì¸ ê²½ìš°
        disease_info = df.loc[df['Symptoms'] == best_match[0]]
        disease_name = disease_info['Disease'].values[0]
        symptoms = disease_info['Symptoms'].values[0]
        description = disease_info['Description'].values[0]
        return f"ì§ˆë³‘: {disease_name}\nì¦ìƒ: {symptoms}\nì„¤ëª…: {description}"
    return "í•´ë‹¹ ì¦ìƒì— ëŒ€í•œ ì§ˆë³‘ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


# Gradio_Taps

# In[59]:


# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    with gr.Tabs():
        
    # 1. csvê¸°ë°˜ ë“œë¡­ë‹¤ìš´ ê°„ë‹¨í˜•ì‹
        with gr.TabItem(" ğŸš‘ ê³ ì–‘ì´ ì§ˆë³‘ ê°„ë‹¨ ì˜ˆì¸¡ ğŸš‘ "):
            gr.Markdown("""
                    <div style="text-align: center; font-size: 24px; font-weight: bold; color: #7193BD; margin:30px;"> 
                    ğŸ¾ ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ ì„œë¹„ìŠ¤ - ë“œë¡­ë‹¤ìš´ğŸ¾ </div>
                """)
            gr.Markdown("""
                    <div style="text-align: right; color: #868e96; font-weight: bold; margin-bottom: 30px;">
                    ê³ ì–‘ì´ì˜ ì¦ìƒ ì¤‘ ë¹„ìŠ·í•œ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.
                    </div>
                """)
            
            # ì¦ìƒ ëª©ë¡ì„ ë“œë¡­ë‹¤ìš´ì— ì¶”ê°€
            symptoms = df['Symptoms'].dropna().unique().tolist()
            
            symptom_dropdown = gr.Dropdown(choices=symptoms, label="ì¦ìƒ ì„ íƒ", interactive=True)
            output_box = gr.Textbox(label="ì§ˆë³‘ ì§„ë‹¨ ê²°ê³¼", lines=10, interactive=False)

            submit_button = gr.Button("ì§„ë‹¨ ìš”ì²­")
            submit_button.click(fn=predict_disease_from_card, inputs=symptom_dropdown, outputs=output_box)

    # 2. ì¦ìƒ/ë‚˜ì´/ì¦ìƒê¸°ê°„ ì–‘ì‹ì„¤ì •ì— ë”°ë¥¸ í˜•ì‹
        with gr.TabItem(" ğŸ” ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ ğŸ” "):
            gr.Markdown("""
                    <div style="text-align: center; font-size: 24px; font-weight: bold; color: #7193BD; margin: 30px;"> 
                    ğŸ¾ ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ ì„œë¹„ìŠ¤ - ì–‘ì‹ ê¸°ë°˜ğŸ¾ </div>
                """)
            gr.Markdown("""
                    <div style="text-align: right; color: #868e96; font-weight: bold; margin-bottom: 30px;">
                    ê³ ì–‘ì´ì˜ ìƒíƒœë¥¼ ìì„¸íˆ ì…ë ¥í•˜ì„¸ìš”.
                    </div>
                """)

            symptom_input = gr.Textbox(label="ì¦ìƒ ì…ë ¥", placeholder="ì˜ˆ: ê³ ì–‘ì´ê°€ ë°¥ì„ ì•ˆ ë¨¹ê³  êµ¬í† ë¥¼ í•´ìš”.")
            age_input = gr.Number(label="ê³ ì–‘ì´ ë‚˜ì´ (ë…„)", value=3)
            duration_input = gr.Number(label="ì¦ìƒ ì§€ì† ê¸°ê°„ (ì¼)", value=1)

            output_box = gr.Textbox(label="ì§ˆë³‘ ì§„ë‹¨ ê²°ê³¼", lines=10, interactive=False)
            submit_button = gr.Button("ì§„ë‹¨ ìš”ì²­")
            submit_button.click(fn=predict_from_form, inputs=[symptom_input, age_input, duration_input], outputs=output_box)


    # 3. ììœ í˜• ì±—ë´‡
        with gr.TabItem(" ğŸ¤– ê³ ì–‘ì´ ì§ˆë³‘ ììœ í˜• AI ì±—ë´‡ ğŸ¤– "):
            with gr.Column():
                gr.HTML("""
                    <div style="text-align: center; font-size: 24px; color: #7193BD; font-weight: bold; margin-bottom: 15px;">ğŸ˜ºê³ ì–‘ì´ ì§ˆë³‘ ìê°€ì§„ë‹¨ ì±—ë´‡ğŸ˜º</div>
                """)
                chatbot = gr.ChatInterface(
                    fn=conversational_chat,
                    examples=[
                        "ê³ ì–‘ì´ê°€ ì‹ì‚¬ ê±°ë¶€í•˜ê³  ì¹¨ì„ ë§ì´ í˜ë ¤ìš”.",
                        "ê³ ì–‘ì´ê°€ ë³µë¶€ì— ì´ìƒì´ ìˆëŠ” ê²ƒ ê°™ì•„ìš”.",
                        "ê³ ì–‘ì´ê°€ ì‹ì‚¬ë¥¼ ê±°ë¶€í•´ìš”.",
                        "ê³ ì–‘ì´ê°€ ë¬´ê¸°ë ¥í•˜ê³  ê·¸ë£¨ë°ì„ ê³¼í•˜ê²Œí•´ìš”",
                        "ê³ ì–‘ì´ì—ê²Œ í”í•œ ì§ˆë³‘ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                        "ê³ ì–‘ì´ì—ê²Œ ì•ˆì¢‹ì€ ì‹ë¬¼ì€ ë¬´ì—‡ì¸ê°€ìš”?"
                    ]
                )


# In[60]:


demo.launch(server_port=7861, server_name="0.0.0.0", share=True)


# In[58]:


demo.close()

