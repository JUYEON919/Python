{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê³ ì–‘ì´ ì§ˆë³‘ê´€ë ¨ ì±—ë´‡\n",
    "(ì±—ë´‡,ì›Œë“œí´ë¼ìš°ë“œ//ì´í›„ì— ë‹¤ë¥¸ê±¸ ì¶”ê°€í•´ë³´ê¸° ì¼ë‹¨ ë‘ê°œë¶€í„°)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì±—ë´‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì›Œë“œí´ë¼ìš°ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\HJY310\\.venv\\lib\\site-packages\\gradio\\components\\chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from fuzzywuzzy import process\n",
    "import re\n",
    "\n",
    "# CSV íŒŒì¼ ë¡œë“œ (ê²½ë¡œë¥¼ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½í•´ì£¼ì„¸ìš”)\n",
    "df = pd.read_csv(\"./data/cat_diseases.csv\", encoding='CP949')\n",
    "\n",
    "# Symptomsì™€ Descriptionì„ í•©ì³ì„œ inputs ì»¬ëŸ¼ ìƒì„±\n",
    "#df['inputs'] = df['Symptoms'] + \" \" + df['Description']\n",
    "df['inputs'] = df['Symptoms'].fillna('') + \" \" + df['Description'].fillna('')\n",
    "\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í• \n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(\"\\n\".join(df['inputs']))\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "\n",
    "# ChatOllama ëª¨ë¸ ì´ˆê¸°í™” (temperature ê°’ ì¡°ì •)\n",
    "#llm = ChatOllama(model=\"gemma2\", temperature=0.0)\n",
    "try:\n",
    "    llm = ChatOllama(model=\"gemma2\", temperature=0.0)\n",
    "except Exception as e:\n",
    "    print(f\"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}\")\n",
    "    llm = None\n",
    "\n",
    "\n",
    "# Question Generation Prompt\n",
    "question_generator_template = \"\"\"\n",
    "ì´ì „ ëŒ€í™” ë‚´ì—­ê³¼ ìƒˆë¡œìš´ ì‚¬ìš©ì ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ê²€ìƒ‰ì„ ìœ„í•œ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¿”ì„œ ìƒì„±í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì´ì „ ëŒ€í™” ë‚´ì—­:\n",
    "{chat_history}\n",
    "\n",
    "ìƒˆë¡œìš´ ì‚¬ìš©ì ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ë…ë¦½ì ì¸ ì§ˆë¬¸:\n",
    "\"\"\"\n",
    "QUESTION_GENERATOR_PROMPT = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=question_generator_template)\n",
    "\n",
    "# Combine Documents Prompt (í•œêµ­ì–´ ì§€ì‹œ ê°•í™” ë° í˜•ì‹ ëª…í™•í™”)\n",
    "combine_documents_template = \"\"\"\n",
    "ë‹¹ì‹ ì€ ê²½í—˜ ë§ì€ ê³ ì–‘ì´ ìˆ˜ì˜ì‚¬ì…ë‹ˆë‹¤. **ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ì—¬, ì–´ìƒ‰í•¨ì´ ì—†ë„ë¡ ë§¤ìš° ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì—¬ ì •ì¤‘í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.** ì™¸êµ­ì–´ë‚˜ ì–´ìƒ‰í•œ í•œêµ­ì–´ í‘œí˜„ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì‹­ì‹œì˜¤.\n",
    "\n",
    "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "1.  ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€ì„ **í•œêµ­ì–´ë¡œ** ì œê³µí•©ë‹ˆë‹¤.\n",
    "2.  í•„ìš”í•œ ê²½ìš°, ì¶”ê°€ì ì¸ ì§ˆë¬¸ì„ í†µí•´ ìƒí™©ì„ ëª…í™•íˆ íŒŒì•…í•˜ë ¤ê³  ë…¸ë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¦ìƒì˜ ê¸°ê°„, ì‹¬ê°ì„±, ë‹¤ë¥¸ ë™ë°˜ ì¦ìƒ ë“±ì„ **í•œêµ­ì–´ë¡œ, ë¶€ë“œëŸ½ê²Œ** ë¬¼ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜ˆ: \"í˜¹ì‹œ ì–¸ì œë¶€í„° ê·¸ëŸ¬ì…¨ë‚˜ìš”?\", \"ë‹¤ë¥¸ ë¶ˆí¸í•œ ì ì€ ì—†ìœ¼ì‹ ê°€ìš”?\")\n",
    "3.  ê°€ëŠ¥í•œ ì›ì¸ ì§ˆë³‘ì„ ì–¸ê¸‰í•˜ê³ , ê° ì§ˆë³‘ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…ì„ **í•œêµ­ì–´ë¡œ, ì´í•´í•˜ê¸° ì‰½ê²Œ** ì œê³µí•©ë‹ˆë‹¤.\n",
    "4.  ì§‘ì—ì„œ í•  ìˆ˜ ìˆëŠ” ì¡°ì¹˜ì™€ ë™ë¬¼ë³‘ì› ë°©ë¬¸ì´ í•„ìš”í•œ ê²½ìš°ë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ **í•œêµ­ì–´ë¡œ, ì¹œì ˆí•˜ê²Œ** ì•ˆë‚´í•©ë‹ˆë‹¤.\n",
    "5.  ì ˆëŒ€ ì§„ë‹¨ì´ë‚˜ ì²˜ë°©ì„ ë‚´ë¦¬ì§€ ì•Šê³ , ë°˜ë“œì‹œ ë™ë¬¼ë³‘ì›ì— ë°©ë¬¸í•˜ì—¬ ì •í™•í•œ ì§„ë£Œë¥¼ ë°›ì„ ê²ƒì„ **í•œêµ­ì–´ë¡œ, ì •ì¤‘í•˜ê²Œ** ê¶Œì¥í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê²€ìƒ‰ëœ ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ì‚¬ìš©ì ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ë‹µë³€:\n",
    "\"\"\"\n",
    "COMBINE_DOCUMENTS_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=combine_documents_template)\n",
    "\n",
    "# Chain ìƒì„±\n",
    "question_generator = LLMChain(llm=llm, prompt=QUESTION_GENERATOR_PROMPT)\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=COMBINE_DOCUMENTS_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain(\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 1}),\n",
    "    combine_docs_chain=doc_chain,\n",
    "    question_generator=question_generator,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "# ì…ë ¥í•œ ì±„íŒ… ê³µë°±ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ì ì˜ˆì™¸ì²˜ë¦¬\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.replace(\" \", \"\")\n",
    "    return text.lower()\n",
    "\n",
    "# ì§ˆë³‘ ì •ë³´ ê²€ìƒ‰ í•¨ìˆ˜\n",
    "def search_disease_info(message):\n",
    "    best_match = process.extractOne(message, df['inputs'])\n",
    "    if best_match and best_match[1] > 80:\n",
    "        disease_info = df.loc[df['inputs'] == best_match[0]]\n",
    "        disease_name = disease_info['Disease'].values[0]\n",
    "        symptoms = disease_info['Symptoms'].values[0]\n",
    "        description = disease_info['Description'].values[0]\n",
    "        emergency = disease_info['Emergency'].values[0]\n",
    "        return f\"ì§ˆë³‘: {disease_name}\\nì¦ìƒ: {symptoms}\\nì„¤ëª…: {description}\\nì‘ê¸‰ìƒí™© ì—¬ë¶€: {emergency}\"\n",
    "    return \"í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "# ê³ ì–‘ì´ ì§ˆë³‘ì— ëŒ€í•œ ì •ë³´ ì œê³µí•˜ëŠ” ì±„íŒ… í•¨ìˆ˜\n",
    "def chat(message, history):\n",
    "    cleaned_message = clean_text(message)\n",
    "    disease_info = search_disease_info(message)\n",
    "    return disease_info\n",
    "\n",
    "# Conversational Chat í•¨ìˆ˜ êµ¬í˜„ (chat_history ê´€ë¦¬)\n",
    "#def conversational_chat(message, history):\n",
    "#    global chat_history\n",
    "#    result = qa({\"question\": message, \"chat_history\": chat_history})\n",
    "#    chat_history = [(message, result[\"answer\"])]\n",
    "#    return result[\"answer\"]\n",
    "\n",
    "def conversational_chat(message, history=[]):\n",
    "    result = qa({\"question\": message, \"chat_history\": history})\n",
    "    history.append((message, result[\"answer\"]))\n",
    "    return result[\"answer\"]\n",
    "\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì • (CSS ë° components í™œìš©)\n",
    "with gr.Blocks(css=\"\"\"\n",
    ".chat-interface-container {\n",
    "    height: 650px;\n",
    "    max-height: 800px;\n",
    "    overflow-y: auto;\n",
    "    border: 3px solid #7193BD;\n",
    "    padding: 15px;\n",
    "    border-radius: 15px;\n",
    "}\n",
    ".message {\n",
    "    background-color: #F1E6BF !important;\n",
    "    white-space: pre-wrap !important; /* ê¸´ í…ìŠ¤íŠ¸ ì¤„ë°”ê¿ˆ ì²˜ë¦¬ */\n",
    "}\n",
    ".message.user {\n",
    "    background-color: #F1E6BF !important;\n",
    "    padding: 10px;\n",
    "    border-radius: 15px;\n",
    "}\n",
    ".message.ai {\n",
    "    background-color: #BED8EB !important;\n",
    "    padding: 10px;\n",
    "    border-radius: 15px;\n",
    "}\n",
    ".chat-title {\n",
    "    font-size: 35px !important; /* ê¸€ì”¨ í¬ê¸° ì¡°ì • */\n",
    "    color: #93B9D6 !important; /* ê¸€ì”¨ ìƒ‰ìƒ ì¡°ì • - ì—¬ê¸°ì„œ ì„¸ë¯¸ì½œë¡  ì• ê³µë°± ì œê±° */\n",
    "    text-align: center;\n",
    "    margin-bottom: 20px;\n",
    "    font-weight: bold !important; /* ê¸€ì ë‘ê»˜ ì¡°ì • */\n",
    "}\n",
    ".description { \n",
    "    text-align: right !important; /* í…ìŠ¤íŠ¸ ì˜¤ë¥¸ìª½ ì •ë ¬ */ \n",
    "    color: #6D6875 !important;\n",
    "    font-weight: bold !important;\n",
    "    margin-bottom: 20px; /* ì¶”ê°€ë¡œ ê°„ê²© ì¡°ì • */\n",
    "}\n",
    "\"\"\") as demo:\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\" ê³ ì–‘ì´ ì§ˆë³‘ AI ì±—ë´‡ \"):\n",
    "            chat_demo = gr.ChatInterface(fn=chat, examples=[\"ì¹¨ í˜ë¦¼, ì‹ì‚¬ ê±°ë¶€, ì… ëƒ„ìƒˆ\", \"ì¦ì€ ë°°ë³€, ë¬¼ ê°™ì€ ë³€, íƒˆìˆ˜\", \"êµ¬í† , ì„¤ì‚¬, ì²´ì¤‘ ê°ì†Œ.\"], title=\"ê³ ì–‘ì´ ì§ˆë³‘ AI ì±—ë´‡\", description=\"ê³ ì–‘ì´ì˜ ì§ˆë³‘ê³¼ ì¦ìƒì— ëŒ€í•´ ì§ˆë¬¸í•˜ë©´, ì§ˆë³‘ ì •ë³´ì™€ ì‘ê¸‰ìƒí™© ì—¬ë¶€ë¥¼ AIê°€ ì•Œë ¤ì¤ë‹ˆë‹¤.\")\n",
    "            chat_demo\n",
    "\n",
    "        with gr.TabItem(\" ê³ ì–‘ì´ ì§ˆë³‘ ììœ í˜• AI ì±—ë´‡ \"):\n",
    "            with gr.Column(elem_classes=[\"chat-interface-container\"]):\n",
    "                # Div ì¶”ê°€í•˜ì—¬ ìŠ¤íƒ€ì¼ ì ìš©\n",
    "                gr.HTML(\"\"\"\n",
    "                    <div class=\"chat-title\">ê³ ì–‘ì´ ì§ˆë³‘ ìê°€ì§„ë‹¨ ì±—ë´‡</div>\n",
    "                    <div class=\"description\">ê³ ì–‘ì´ ì§ˆë³‘ì— ëŒ€í•´ ììœ ë¡­ê²Œ ì§ˆë¬¸í•˜ê³  ë‹µë³€ì„ ë°›ì•„ë³´ì„¸ìš”.</div>\n",
    "                \"\"\")\n",
    "                chatbot = gr.ChatInterface(\n",
    "                    fn=conversational_chat,\n",
    "                    examples=[\n",
    "                        \"ê³ ì–‘ì´ê°€ ì‹ì‚¬ ê±°ë¶€í•˜ê³  ì¹¨ì„ ë§ì´ í˜ë ¤ìš”.\",\n",
    "                        \"ê³ ì–‘ì´ê°€ ë³µë¶€ì— ì´ìƒì´ ìˆëŠ” ê²ƒ ê°™ì•„ìš”.\",\n",
    "                        \"ê³ ì–‘ì´ì˜ ì¦ìƒì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "                        \"ê³ ì–‘ì´ ë²”ë°±í˜ˆêµ¬ ê°ì†Œì¦ì˜ ì›ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "                        \"ê³ ì–‘ì´ì—ê²Œ í”í•œ ì§ˆë³‘ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "                    ]\n",
    "                ) # components ì¸ì ì œê±°\n",
    "\n",
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tapíƒ­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\HJY310\\.venv\\lib\\site-packages\\gradio\\components\\chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from fuzzywuzzy import process\n",
    "import re\n",
    "\n",
    "# CSV íŒŒì¼ ë¡œë“œ (ê²½ë¡œë¥¼ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½í•´ì£¼ì„¸ìš”)\n",
    "df = pd.read_csv(\"./data/cat_diseases.csv\", encoding='CP949')\n",
    "\n",
    "# Symptomsì™€ Descriptionì„ í•©ì³ì„œ inputs ì»¬ëŸ¼ ìƒì„±\n",
    "df['inputs'] = df['Symptoms'].fillna('') + \" \" + df['Description'].fillna('')\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í• \n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(\"\\n\".join(df['inputs']))\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "\n",
    "# ChatOllama ëª¨ë¸ ì´ˆê¸°í™” (temperature ê°’ ì¡°ì •)\n",
    "try:\n",
    "    llm = ChatOllama(model=\"gemma2\", temperature=0.0)\n",
    "except Exception as e:\n",
    "    print(f\"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}\")\n",
    "    llm = None\n",
    "\n",
    "# Question Generation Prompt\n",
    "question_generator_template = \"\"\"\n",
    "ì´ì „ ëŒ€í™” ë‚´ì—­ê³¼ ìƒˆë¡œìš´ ì‚¬ìš©ì ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ê²€ìƒ‰ì„ ìœ„í•œ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¿”ì„œ ìƒì„±í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì´ì „ ëŒ€í™” ë‚´ì—­:\n",
    "{chat_history}\n",
    "\n",
    "ìƒˆë¡œìš´ ì‚¬ìš©ì ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ë…ë¦½ì ì¸ ì§ˆë¬¸:\n",
    "\"\"\"\n",
    "QUESTION_GENERATOR_PROMPT = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=question_generator_template)\n",
    "\n",
    "# Combine Documents Prompt (í•œêµ­ì–´ ì§€ì‹œ ê°•í™” ë° í˜•ì‹ ëª…í™•í™”)\n",
    "combine_documents_template = \"\"\"\n",
    "ë‹¹ì‹ ì€ ê²½í—˜ ë§ì€ ê³ ì–‘ì´ ìˆ˜ì˜ì‚¬ì…ë‹ˆë‹¤. **ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ì—¬, ì–´ìƒ‰í•¨ì´ ì—†ë„ë¡ ë§¤ìš° ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì—¬ ì •ì¤‘í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.** ì™¸êµ­ì–´ë‚˜ ì–´ìƒ‰í•œ í•œêµ­ì–´ í‘œí˜„ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì‹­ì‹œì˜¤.\n",
    "\n",
    "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "1.  ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€ì„ **í•œêµ­ì–´ë¡œ** ì œê³µí•©ë‹ˆë‹¤.\n",
    "2.  í•„ìš”í•œ ê²½ìš°, ì¶”ê°€ì ì¸ ì§ˆë¬¸ì„ í†µí•´ ìƒí™©ì„ ëª…í™•íˆ íŒŒì•…í•˜ë ¤ê³  ë…¸ë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¦ìƒì˜ ê¸°ê°„, ì‹¬ê°ì„±, ë‹¤ë¥¸ ë™ë°˜ ì¦ìƒ ë“±ì„ **í•œêµ­ì–´ë¡œ, ë¶€ë“œëŸ½ê²Œ** ë¬¼ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜ˆ: \"í˜¹ì‹œ ì–¸ì œë¶€í„° ê·¸ëŸ¬ì…¨ë‚˜ìš”?\", \"ë‹¤ë¥¸ ë¶ˆí¸í•œ ì ì€ ì—†ìœ¼ì‹ ê°€ìš”?\")\n",
    "3.  ê°€ëŠ¥í•œ ì›ì¸ ì§ˆë³‘ì„ ì–¸ê¸‰í•˜ê³ , ê° ì§ˆë³‘ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…ì„ **í•œêµ­ì–´ë¡œ, ì´í•´í•˜ê¸° ì‰½ê²Œ** ì œê³µí•©ë‹ˆë‹¤.\n",
    "4.  ì§‘ì—ì„œ í•  ìˆ˜ ìˆëŠ” ì¡°ì¹˜ì™€ ë™ë¬¼ë³‘ì› ë°©ë¬¸ì´ í•„ìš”í•œ ê²½ìš°ë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ **í•œêµ­ì–´ë¡œ, ì¹œì ˆí•˜ê²Œ** ì•ˆë‚´í•©ë‹ˆë‹¤.\n",
    "5.  ì ˆëŒ€ ì§„ë‹¨ì´ë‚˜ ì²˜ë°©ì„ ë‚´ë¦¬ì§€ ì•Šê³ , ë°˜ë“œì‹œ ë™ë¬¼ë³‘ì›ì— ë°©ë¬¸í•˜ì—¬ ì •í™•í•œ ì§„ë£Œë¥¼ ë°›ì„ ê²ƒì„ **í•œêµ­ì–´ë¡œ, ì •ì¤‘í•˜ê²Œ** ê¶Œì¥í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "6.  ì‚¬ìš©ìê°€ ê±±ì •í•˜ê±°ë‚˜ ë¶ˆì•ˆí•œ ê°ì •ì„ ëŠë‚„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, **ìœ„ë¡œ**ì™€ **ì•ˆì‹¬**ì„ ì£¼ëŠ” ë§ì„ í¬í•¨í•˜ì—¬ ì‚¬ìš©ìê°€ ë” í¸ì•ˆí•˜ê²Œ ëŠë¼ë„ë¡ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê²€ìƒ‰ëœ ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ì‚¬ìš©ì ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ë‹µë³€:\n",
    "\"\"\"\n",
    "COMBINE_DOCUMENTS_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=combine_documents_template)\n",
    "\n",
    "# Chain ìƒì„±\n",
    "question_generator = LLMChain(llm=llm, prompt=QUESTION_GENERATOR_PROMPT)\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=COMBINE_DOCUMENTS_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain(\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 1}),\n",
    "    combine_docs_chain=doc_chain,\n",
    "    question_generator=question_generator,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "# ì…ë ¥í•œ ì±„íŒ… ê³µë°±ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ì ì˜ˆì™¸ì²˜ë¦¬\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.replace(\" \", \"\")\n",
    "    return text.lower()\n",
    "\n",
    "# ì§ˆë³‘ ì •ë³´ ê²€ìƒ‰ í•¨ìˆ˜\n",
    "def search_disease_info(message):\n",
    "    best_match = process.extractOne(message, df['inputs'])\n",
    "    if best_match and best_match[1] > 80:\n",
    "        disease_info = df.loc[df['inputs'] == best_match[0]]\n",
    "        disease_name = disease_info['Disease'].values[0]\n",
    "        symptoms = disease_info['Symptoms'].values[0]\n",
    "        description = disease_info['Description'].values[0]\n",
    "        emergency = disease_info['Emergency'].values[0]\n",
    "        return f\"ì§ˆë³‘: {disease_name}\\nì¦ìƒ: {symptoms}\\nì„¤ëª…: {description}\\nì‘ê¸‰ìƒí™© ì—¬ë¶€: {emergency}\"\n",
    "    return \"í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "# ê³ ì–‘ì´ ì§ˆë³‘ì— ëŒ€í•œ ì •ë³´ ì œê³µí•˜ëŠ” ì±„íŒ… í•¨ìˆ˜\n",
    "def chat(message, history):\n",
    "    cleaned_message = clean_text(message)\n",
    "    disease_info = search_disease_info(message)\n",
    "    return disease_info\n",
    "\n",
    "# Conversational Chat í•¨ìˆ˜ êµ¬í˜„ (chat_history ê´€ë¦¬)\n",
    "def conversational_chat(message, history=[]):\n",
    "#def conversational_chat(message, history):\n",
    "    result = qa({\"question\": message, \"chat_history\": history})\n",
    "    history.append((message, result[\"answer\"]))\n",
    "    return result[\"answer\"]\n",
    "\n",
    "#def conversational_chat(message, history):\n",
    "    # ìƒˆë¡œìš´ ì§ˆë¬¸ì´ ì¶”ê°€ë˜ê¸° ì „ì— ê¸°ì¡´ historyì—ì„œ ë™ì¼í•œ ì§ˆë¬¸ì´ ìˆëŠ”ì§€ ì²´í¬\n",
    " #   if message not in [m for m, _ in history]:\n",
    " #       history.append((message, \"\"))\n",
    "    \n",
    "    # QA ìˆ˜í–‰\n",
    "  #  result = qa({\"question\": message, \"chat_history\": history})\n",
    "    \n",
    "    # ìƒˆë¡œìš´ ì‘ë‹µì„ historyì— ì¶”ê°€\n",
    "  #  history[-1] = (message, result['answer'])\n",
    "    \n",
    "  #  return result['answer']\n",
    "\n",
    "# ì„¸ ë²ˆì§¸ íƒ­ìš© í•¨ìˆ˜\n",
    "custom_prompt = \"\"\"\n",
    "ë‹¹ì‹ ì€ ê³ ì–‘ì´ ì§ˆë³‘ì— ëŒ€í•´ ì „ë¬¸ì ì¸ ìˆ˜ì˜ì‚¬ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "ì•„ë˜ ì¦ìƒì— ê¸°ë°˜í•˜ì—¬ ê°€ëŠ¥í•œ ì§ˆë³‘ê³¼ ê´€ë ¨ ì •ë³´ë¥¼ ì‘ì„±í•˜ì„¸ìš”:\n",
    "\n",
    "ì¦ìƒ: {symptom}\n",
    "\n",
    "ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\"\"\"\n",
    "PROMPT_TEMPLATE = PromptTemplate(input_variables=[\"symptom\"], template=custom_prompt)\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT_TEMPLATE)\n",
    "\n",
    "def predict_from_form(symptom, age, duration):\n",
    "    prompt = f\"\"\"\n",
    "    ê³ ì–‘ì´ ë‚˜ì´: {age}\n",
    "    ì¦ìƒ ë°œìƒ ê¸°ê°„: {duration}ì¼\n",
    "    ì¦ìƒ: {symptom}\n",
    "    \n",
    "    ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ëŠ¥í•œ ì§ˆë³‘ê³¼ ëŒ€ì²˜ ë°©ë²•ì„ ì‘ì„±í•˜ì„¸ìš”.\n",
    "    \"\"\"\n",
    "    response = chain.run(symptom=prompt)\n",
    "    return response\n",
    "\n",
    "# ë“œë¡­ë‹¤ìš´ ê¸°ë°˜ ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "def predict_disease_from_card(symptom):\n",
    "    best_match = process.extractOne(symptom, df['Symptoms'].dropna())\n",
    "    if best_match and best_match[1] > 80:\n",
    "        disease_info = df.loc[df['Symptoms'] == best_match[0]]\n",
    "        disease_name = disease_info['Disease'].values[0]\n",
    "        symptoms = disease_info['Symptoms'].values[0]\n",
    "        description = disease_info['Description'].values[0]\n",
    "        return f\"ì§ˆë³‘: {disease_name}\\nì¦ìƒ: {symptoms}\\nì„¤ëª…: {description}\"\n",
    "    return \"í•´ë‹¹ ì¦ìƒì— ëŒ€í•œ ì§ˆë³‘ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\" ê³ ì–‘ì´ ì§ˆë³‘ AI ì±—ë´‡ \"):\n",
    "            chat_demo = gr.ChatInterface(\n",
    "                fn=chat, \n",
    "                examples=[\n",
    "                    \"ì¹¨ í˜ë¦¼, ì‹ì‚¬ ê±°ë¶€, ì… ëƒ„ìƒˆ\", \n",
    "                    \"ì¦ì€ ë°°ë³€, ë¬¼ ê°™ì€ ë³€, íƒˆìˆ˜\", \n",
    "                    \"êµ¬í† , ì„¤ì‚¬, ì²´ì¤‘ ê°ì†Œ.\"\n",
    "                ],\n",
    "                title=\"ê³ ì–‘ì´ ì§ˆë³‘ AI ì±—ë´‡\",\n",
    "                description=\"ê³ ì–‘ì´ì˜ ì§ˆë³‘ê³¼ ì¦ìƒì— ëŒ€í•´ ì§ˆë¬¸í•˜ë©´, ì§ˆë³‘ ì •ë³´ì™€ ì‘ê¸‰ìƒí™© ì—¬ë¶€ë¥¼ AIê°€ ì•Œë ¤ì¤ë‹ˆë‹¤.\"\n",
    "            )\n",
    "            chat_demo\n",
    "\n",
    "        with gr.TabItem(\" ê³ ì–‘ì´ ì§ˆë³‘ ììœ í˜• AI ì±—ë´‡ \"):\n",
    "            with gr.Column():\n",
    "                gr.HTML(\"\"\"\n",
    "                    <div style=\"text-align: center; font-size: 24px; color: #7193BD; font-weight: bold; margin-bottom: 15px;\">ê³ ì–‘ì´ ì§ˆë³‘ ìê°€ì§„ë‹¨ ì±—ë´‡</div>\n",
    "                \"\"\")\n",
    "                chatbot = gr.ChatInterface(\n",
    "                    fn=conversational_chat,\n",
    "                    examples=[\n",
    "                        \"ê³ ì–‘ì´ê°€ ì‹ì‚¬ ê±°ë¶€í•˜ê³  ì¹¨ì„ ë§ì´ í˜ë ¤ìš”.\",\n",
    "                        \"ê³ ì–‘ì´ê°€ ë³µë¶€ì— ì´ìƒì´ ìˆëŠ” ê²ƒ ê°™ì•„ìš”.\",\n",
    "                        \"ê³ ì–‘ì´ì˜ ì¦ìƒì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "                        \"ê³ ì–‘ì´ ë²”ë°±í˜ˆêµ¬ ê°ì†Œì¦ì˜ ì›ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "                        \"ê³ ì–‘ì´ì—ê²Œ í”í•œ ì§ˆë³‘ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "        with gr.TabItem(\" ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ - ì–‘ì‹ ê¸°ë°˜ \"):\n",
    "            gr.Markdown(\"## ğŸ¾ ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ ì„œë¹„ìŠ¤ - ì–‘ì‹ ê¸°ë°˜\")\n",
    "            gr.Markdown(\"### ê³ ì–‘ì´ì˜ ìƒíƒœë¥¼ ìì„¸íˆ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "            symptom_input = gr.Textbox(label=\"ì¦ìƒ ì…ë ¥\", placeholder=\"ì˜ˆ: ê³ ì–‘ì´ê°€ ë°¥ì„ ì•ˆ ë¨¹ê³  êµ¬í† ë¥¼ í•´ìš”.\")\n",
    "            age_input = gr.Number(label=\"ê³ ì–‘ì´ ë‚˜ì´ (ë…„)\", value=3)\n",
    "            duration_input = gr.Number(label=\"ì¦ìƒ ì§€ì† ê¸°ê°„ (ì¼)\", value=1)\n",
    "\n",
    "            output_box = gr.Textbox(label=\"ì§ˆë³‘ ì§„ë‹¨ ê²°ê³¼\", lines=10, interactive=False)\n",
    "            submit_button = gr.Button(\"ì§„ë‹¨ ìš”ì²­\")\n",
    "            submit_button.click(fn=predict_from_form, inputs=[symptom_input, age_input, duration_input], outputs=output_box)\n",
    "\n",
    "        with gr.TabItem(\" ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ - ë“œë¡­ë‹¤ìš´ ê¸°ë°˜ \"):\n",
    "            gr.Markdown(\"## ğŸ¾ ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ ì„œë¹„ìŠ¤ - ë“œë¡­ë‹¤ìš´ ê¸°ë°˜\")\n",
    "            gr.Markdown(\"### ê³ ì–‘ì´ì˜ ì¦ìƒ ì¤‘ í•˜ë‚˜ë¥¼ ë“œë¡­ë‹¤ìš´ì—ì„œ ì„ íƒí•˜ì„¸ìš”.\")\n",
    "            \n",
    "            # ì¦ìƒ ëª©ë¡ì„ ë“œë¡­ë‹¤ìš´ì— ì¶”ê°€\n",
    "            symptoms = df['Symptoms'].dropna().unique().tolist()\n",
    "            \n",
    "            symptom_dropdown = gr.Dropdown(choices=symptoms, label=\"ì¦ìƒ ì„ íƒ\", interactive=True)\n",
    "            output_box = gr.Textbox(label=\"ì§ˆë³‘ ì§„ë‹¨ ê²°ê³¼\", lines=10, interactive=False)\n",
    "\n",
    "            submit_button = gr.Button(\"ì§„ë‹¨ ìš”ì²­\")\n",
    "            submit_button.click(fn=predict_disease_from_card, inputs=symptom_dropdown, outputs=output_box)\n",
    "\n",
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-28\\AppData\\Local\\Temp\\ipykernel_12896\\1010872933.py:55: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(input=user_input)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# CSV íŒŒì¼ ë¡œë“œ (ê²½ë¡œë¥¼ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½)\n",
    "df = pd.read_csv(\"./data/cat_diseases.csv\", encoding='CP949')\n",
    "\n",
    "# Symptomsì™€ Description í•©ì¹˜ê¸°\n",
    "df['inputs'] = df['Symptoms'].fillna('') + \" \" + df['Description'].fillna('')\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í• \n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(\"\\n\".join(df['inputs']))\n",
    "\n",
    "# ì„ë² ë”© ë° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "\n",
    "# ChatOllama ëª¨ë¸ ì´ˆê¸°í™”\n",
    "try:\n",
    "    llm = ChatOllama(model=\"gemma2\", temperature=0.0)\n",
    "except Exception as e:\n",
    "    print(f\"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}\")\n",
    "    llm = None\n",
    "\n",
    "# ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì§ˆë³‘ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±\n",
    "custom_prompt = \"\"\"\n",
    "ë‹¹ì‹ ì€ ê³ ì–‘ì´ ì§ˆë³‘ì— ëŒ€í•´ ì „ë¬¸ì ì¸ ìˆ˜ì˜ì‚¬ ì—­í• ì„ í•©ë‹ˆë‹¤. \n",
    "ì‚¬ìš©ìê°€ ì œê³µí•œ ì¦ìƒì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì •ë³´ë¥¼ ì‘ì„±í•˜ì„¸ìš”:\n",
    "\n",
    "1. í•´ë‹¹ ì¦ìƒì— ëŒ€í•´ ê°€ëŠ¥í•œ ì§ˆë³‘ ëª©ë¡ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "2. ê° ì§ˆë³‘ì˜ ì¦ìƒê³¼ ê°„ëµí•œ ì„¤ëª…ì„ ì¶”ê°€í•˜ì„¸ìš”.\n",
    "3. ì‘ê¸‰ìƒí™©ì¸ì§€ ì—¬ë¶€ë¥¼ ì‚¬ìš©ìì—ê²Œ ì„¤ëª…í•˜ê³  í•„ìš”í•œ ì¡°ì¹˜ë¥¼ ì•ˆë‚´í•˜ì„¸ìš”.\n",
    "4. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "ì‚¬ìš©ì ì…ë ¥: {input}\n",
    "\"\"\"\n",
    "PROMPT_TEMPLATE = PromptTemplate(\n",
    "    input_variables=[\"input\"], \n",
    "    template=custom_prompt\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT_TEMPLATE)\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜\n",
    "def predict_disease_info(user_input):\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê³ ì–‘ì´ ì¦ìƒì— ëŒ€í•´ LLMì„ ì´ìš©í•´ ë‹µë³€ ìƒì„±.\n",
    "    \"\"\"\n",
    "    # LLMì„ ì‚¬ìš©í•´ ë‹µë³€ ìƒì„±\n",
    "    response = chain.run(input=user_input)\n",
    "    return response\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## ğŸ¾ ê³ ì–‘ì´ ì§ˆë³‘ ì§„ë‹¨ ì„œë¹„ìŠ¤\")\n",
    "    gr.Markdown(\"### ê³ ì–‘ì´ì˜ ì¦ìƒì— ëŒ€í•´ ì…ë ¥í•˜ë©´, ê°€ëŠ¥í•œ ì§ˆë³‘ ì •ë³´ì™€ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            user_input = gr.Textbox(label=\"ê³ ì–‘ì´ ì¦ìƒ ì…ë ¥\", placeholder=\"ì˜ˆ: ê³ ì–‘ì´ê°€ ë°¥ì„ ì•ˆ ë¨¹ê³  êµ¬í† ë¥¼ í•´ìš”.\")\n",
    "        with gr.Column():\n",
    "            output_text = gr.Textbox(label=\"ì§„ë‹¨ ê²°ê³¼\", interactive=False)\n",
    "    \n",
    "    # ë²„íŠ¼ ìƒì„±\n",
    "    submit_button = gr.Button(\"ì§ˆë³‘ ì˜ˆì¸¡í•˜ê¸°\")\n",
    "    submit_button.click(fn=predict_disease_info, inputs=user_input, outputs=output_text)\n",
    "\n",
    "# ì•± ì‹¤í–‰\n",
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv(\"./data/cat_diseases.csv\", encoding='CP949')\n",
    "\n",
    "# ChatOllama ì´ˆê¸°í™”\n",
    "try:\n",
    "    llm = ChatOllama(model=\"gemma2\", temperature=0.0)\n",
    "except Exception as e:\n",
    "    print(f\"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}\")\n",
    "    llm = None\n",
    "\n",
    "# Prompt ì„¤ì •\n",
    "custom_prompt = \"\"\"\n",
    "ë‹¹ì‹ ì€ ê³ ì–‘ì´ ì§ˆë³‘ì— ëŒ€í•´ ì „ë¬¸ì ì¸ ìˆ˜ì˜ì‚¬ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "ì•„ë˜ ì¦ìƒì— ê¸°ë°˜í•˜ì—¬ ê°€ëŠ¥í•œ ì§ˆë³‘ê³¼ ê´€ë ¨ ì •ë³´ë¥¼ ì‘ì„±í•˜ì„¸ìš”:\n",
    "\n",
    "ì¦ìƒ: {symptom}\n",
    "\n",
    "ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\"\"\"\n",
    "PROMPT_TEMPLATE = PromptTemplate(input_variables=[\"symptom\"], template=custom_prompt)\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT_TEMPLATE)\n",
    "\n",
    "# Gradio í•¨ìˆ˜\n",
    "def predict_disease_from_card(selected_symptom):\n",
    "    response = chain.run(symptom=selected_symptom)\n",
    "    return response\n",
    "\n",
    "# Gradio UI êµ¬ì„±\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## ğŸ¾ ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ ì„œë¹„ìŠ¤ - ì¹´ë“œ ê¸°ë°˜\")\n",
    "    gr.Markdown(\"### ê³ ì–‘ì´ì˜ ì¦ìƒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.\")\n",
    "    \n",
    "    # ì¦ìƒ ì„ íƒ ì¹´ë“œ\n",
    "    symptoms = df['Symptoms'].dropna().unique().tolist()\n",
    "    symptom_cards = gr.Radio(symptoms, label=\"ì¦ìƒ ì„ íƒ\", interactive=True)\n",
    "\n",
    "    # ì¶œë ¥\n",
    "    output_box = gr.Textbox(label=\"ì§ˆë³‘ ì§„ë‹¨ ê²°ê³¼\", lines=10, interactive=False)\n",
    "    submit_button = gr.Button(\"ì§„ë‹¨ ìš”ì²­\")\n",
    "\n",
    "    submit_button.click(fn=predict_disease_from_card, inputs=symptom_cards, outputs=output_box)\n",
    "\n",
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv(\"./data/cat_diseases.csv\", encoding='CP949')\n",
    "\n",
    "# ChatOllama ì´ˆê¸°í™”\n",
    "try:\n",
    "    llm = ChatOllama(model=\"gemma2\", temperature=0.0)\n",
    "except Exception as e:\n",
    "    print(f\"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}\")\n",
    "    llm = None\n",
    "\n",
    "# Prompt ì„¤ì •\n",
    "custom_prompt = \"\"\"\n",
    "ë‹¹ì‹ ì€ ê³ ì–‘ì´ ì§ˆë³‘ì— ëŒ€í•´ ì „ë¬¸ì ì¸ ìˆ˜ì˜ì‚¬ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "ì•„ë˜ ì¦ìƒì— ê¸°ë°˜í•˜ì—¬ ê°€ëŠ¥í•œ ì§ˆë³‘ê³¼ ê´€ë ¨ ì •ë³´ë¥¼ ì‘ì„±í•˜ì„¸ìš”:\n",
    "\n",
    "ì¦ìƒ: {symptom}\n",
    "\n",
    "ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\"\"\"\n",
    "PROMPT_TEMPLATE = PromptTemplate(input_variables=[\"symptom\"], template=custom_prompt)\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT_TEMPLATE)\n",
    "\n",
    "def predict_from_form(symptom, age, duration):\n",
    "    prompt = f\"\"\"\n",
    "    ê³ ì–‘ì´ ë‚˜ì´: {age}\n",
    "    ì¦ìƒ ë°œìƒ ê¸°ê°„: {duration}ì¼\n",
    "    ì¦ìƒ: {symptom}\n",
    "    \n",
    "    ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ëŠ¥í•œ ì§ˆë³‘ê³¼ ëŒ€ì²˜ ë°©ë²•ì„ ì‘ì„±í•˜ì„¸ìš”.\n",
    "    \"\"\"\n",
    "    response = chain.run(symptom=prompt)\n",
    "    return response\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## ğŸ¾ ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ ì„œë¹„ìŠ¤ - ì–‘ì‹ ê¸°ë°˜\")\n",
    "    gr.Markdown(\"### ê³ ì–‘ì´ì˜ ìƒíƒœë¥¼ ìì„¸íˆ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    # ì–‘ì‹ ì…ë ¥\n",
    "    symptom_input = gr.Textbox(label=\"ì¦ìƒ ì…ë ¥\", placeholder=\"ì˜ˆ: ê³ ì–‘ì´ê°€ ë°¥ì„ ì•ˆ ë¨¹ê³  êµ¬í† ë¥¼ í•´ìš”.\")\n",
    "    age_input = gr.Number(label=\"ê³ ì–‘ì´ ë‚˜ì´ (ë…„)\", value=3)\n",
    "    duration_input = gr.Number(label=\"ì¦ìƒ ì§€ì† ê¸°ê°„ (ì¼)\", value=1)\n",
    "\n",
    "    output_box = gr.Textbox(label=\"ì§ˆë³‘ ì§„ë‹¨ ê²°ê³¼\", lines=10, interactive=False)\n",
    "    submit_button = gr.Button(\"ì§„ë‹¨ ìš”ì²­\")\n",
    "\n",
    "    submit_button.click(fn=predict_from_form, inputs=[symptom_input, age_input, duration_input], outputs=output_box)\n",
    "\n",
    "\n",
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\HJY310\\.venv\\lib\\site-packages\\gradio\\components\\chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* Running on public URL: https://7e63215a51307fd172.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7e63215a51307fd172.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from fuzzywuzzy import process\n",
    "import re\n",
    "\n",
    "# CSV íŒŒì¼ ë¡œë“œ (ê²½ë¡œë¥¼ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½í•´ì£¼ì„¸ìš”)\n",
    "df = pd.read_csv(\"./data/cat_diseases.csv\", encoding='CP949')\n",
    "\n",
    "# Symptomsì™€ Descriptionì„ í•©ì³ì„œ inputs ì»¬ëŸ¼ ìƒì„±\n",
    "df['inputs'] = df['Symptoms'].fillna('') + \" \" + df['Description'].fillna('')\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í• \n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(\"\\n\".join(df['inputs']))\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "\n",
    "# ChatOllama ëª¨ë¸ ì´ˆê¸°í™” (temperature ê°’ ì¡°ì •)\n",
    "try:\n",
    "    llm = ChatOllama(model=\"gemma2\", temperature=0.0)\n",
    "except Exception as e:\n",
    "    print(f\"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}\")\n",
    "    llm = None\n",
    "\n",
    "# Question Generation Prompt\n",
    "question_generator_template = \"\"\"\n",
    "ì´ì „ ëŒ€í™” ë‚´ì—­ê³¼ ìƒˆë¡œìš´ ì‚¬ìš©ì ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ê²€ìƒ‰ì„ ìœ„í•œ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¿”ì„œ ìƒì„±í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì´ì „ ëŒ€í™” ë‚´ì—­:\n",
    "{chat_history}\n",
    "\n",
    "ìƒˆë¡œìš´ ì‚¬ìš©ì ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ë…ë¦½ì ì¸ ì§ˆë¬¸:\n",
    "\"\"\"\n",
    "QUESTION_GENERATOR_PROMPT = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=question_generator_template)\n",
    "\n",
    "# Combine Documents Prompt (í•œêµ­ì–´ ì§€ì‹œ ê°•í™” ë° í˜•ì‹ ëª…í™•í™”)\n",
    "combine_documents_template = \"\"\"\n",
    "ë‹¹ì‹ ì€ ê²½í—˜ ë§ì€ ê³ ì–‘ì´ ìˆ˜ì˜ì‚¬ì…ë‹ˆë‹¤. **ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ì—¬, ì–´ìƒ‰í•¨ì´ ì—†ë„ë¡ ë§¤ìš° ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì—¬ ì •ì¤‘í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.** ì™¸êµ­ì–´ë‚˜ ì–´ìƒ‰í•œ í•œêµ­ì–´ í‘œí˜„ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì‹­ì‹œì˜¤.\n",
    "\n",
    "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "1.  ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€ì„ **í•œêµ­ì–´ë¡œ** ì œê³µí•©ë‹ˆë‹¤.\n",
    "2.  í•„ìš”í•œ ê²½ìš°, ì¶”ê°€ì ì¸ ì§ˆë¬¸ì„ í†µí•´ ìƒí™©ì„ ëª…í™•íˆ íŒŒì•…í•˜ë ¤ê³  ë…¸ë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¦ìƒì˜ ê¸°ê°„, ì‹¬ê°ì„±, ë‹¤ë¥¸ ë™ë°˜ ì¦ìƒ ë“±ì„ **í•œêµ­ì–´ë¡œ, ë¶€ë“œëŸ½ê²Œ** ë¬¼ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜ˆ: \"í˜¹ì‹œ ì–¸ì œë¶€í„° ê·¸ëŸ¬ì…¨ë‚˜ìš”?\", \"ë‹¤ë¥¸ ë¶ˆí¸í•œ ì ì€ ì—†ìœ¼ì‹ ê°€ìš”?\")\n",
    "3.  ê°€ëŠ¥í•œ ì›ì¸ ì§ˆë³‘ì„ ì–¸ê¸‰í•˜ê³ , ê° ì§ˆë³‘ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…ì„ **í•œêµ­ì–´ë¡œ, ì´í•´í•˜ê¸° ì‰½ê²Œ** ì œê³µí•©ë‹ˆë‹¤.\n",
    "4.  ì§‘ì—ì„œ í•  ìˆ˜ ìˆëŠ” ì¡°ì¹˜ì™€ ë™ë¬¼ë³‘ì› ë°©ë¬¸ì´ í•„ìš”í•œ ê²½ìš°ë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ **í•œêµ­ì–´ë¡œ, ì¹œì ˆí•˜ê²Œ** ì•ˆë‚´í•©ë‹ˆë‹¤.\n",
    "5.  ì ˆëŒ€ ì§„ë‹¨ì´ë‚˜ ì²˜ë°©ì„ ë‚´ë¦¬ì§€ ì•Šê³ , ë°˜ë“œì‹œ ë™ë¬¼ë³‘ì›ì— ë°©ë¬¸í•˜ì—¬ ì •í™•í•œ ì§„ë£Œë¥¼ ë°›ì„ ê²ƒì„ **í•œêµ­ì–´ë¡œ, ì •ì¤‘í•˜ê²Œ** ê¶Œì¥í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "6.  ì‚¬ìš©ìê°€ ê±±ì •í•˜ê±°ë‚˜ ë¶ˆì•ˆí•œ ê°ì •ì„ ëŠë‚„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, **ìœ„ë¡œ**ì™€ **ì•ˆì‹¬**ì„ ì£¼ëŠ” ë§ì„ í¬í•¨í•˜ì—¬ ì‚¬ìš©ìê°€ ë” í¸ì•ˆí•˜ê²Œ ëŠë¼ë„ë¡ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê²€ìƒ‰ëœ ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ì‚¬ìš©ì ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ë‹µë³€:\n",
    "\"\"\"\n",
    "COMBINE_DOCUMENTS_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=combine_documents_template)\n",
    "\n",
    "# Chain ìƒì„±\n",
    "question_generator = LLMChain(llm=llm, prompt=QUESTION_GENERATOR_PROMPT)\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=COMBINE_DOCUMENTS_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain(\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 1}),\n",
    "    combine_docs_chain=doc_chain,\n",
    "    question_generator=question_generator,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "# ì…ë ¥í•œ ì±„íŒ… ê³µë°±ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ì ì˜ˆì™¸ì²˜ë¦¬\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.replace(\" \", \"\")\n",
    "    return text.lower()\n",
    "\n",
    "# ê³ ì–‘ì´ ì§ˆë³‘ì— ëŒ€í•œ ì •ë³´ ì œê³µí•˜ëŠ” ì±„íŒ… í•¨ìˆ˜\n",
    "def chat(message, history):\n",
    "    cleaned_message = clean_text(message)\n",
    "    disease_info = search_disease_info(message)\n",
    "    return disease_info\n",
    "\n",
    "# Conversational Chat í•¨ìˆ˜ êµ¬í˜„ (chat_history ê´€ë¦¬)\n",
    "def conversational_chat(message, history):\n",
    "    # QA ìˆ˜í–‰\n",
    "    result = qa({\"question\": message, \"chat_history\": history})\n",
    "    \n",
    "    # Gradioê°€ historyë¥¼ ê´€ë¦¬í•˜ë¯€ë¡œ historyë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ\n",
    "    response = result[\"answer\"]\n",
    "    return response  # ì‘ë‹µë§Œ ë°˜í™˜\n",
    "\n",
    "# ì„¸ ë²ˆì§¸ íƒ­ìš© í•¨ìˆ˜\n",
    "custom_prompt = \"\"\"\n",
    "ë‹¹ì‹ ì€ ê³ ì–‘ì´ ì§ˆë³‘ì— ëŒ€í•´ ì „ë¬¸ì ì¸ ìˆ˜ì˜ì‚¬ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "ì•„ë˜ ì¦ìƒì— ê¸°ë°˜í•˜ì—¬ ê°€ëŠ¥í•œ ì§ˆë³‘ê³¼ ê´€ë ¨ ì •ë³´ë¥¼ ì‘ì„±í•˜ì„¸ìš”:\n",
    "\n",
    "ì¦ìƒ: {symptom}\n",
    "\n",
    "ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\"\"\"\n",
    "PROMPT_TEMPLATE = PromptTemplate(input_variables=[\"symptom\"], template=custom_prompt)\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT_TEMPLATE)\n",
    "\n",
    "def predict_from_form(symptom, age, duration):\n",
    "    prompt = f\"\"\"\n",
    "    ê³ ì–‘ì´ ë‚˜ì´: {age}\n",
    "    ì¦ìƒ ë°œìƒ ê¸°ê°„: {duration}ì¼\n",
    "    ì¦ìƒ: {symptom}\n",
    "    \n",
    "    ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ëŠ¥í•œ ì§ˆë³‘ê³¼ ëŒ€ì²˜ ë°©ë²•ì„ ì‘ì„±í•˜ì„¸ìš”.\n",
    "    \"\"\"\n",
    "    response = chain.run(symptom=prompt)\n",
    "    return response\n",
    "\n",
    "# ë“œë¡­ë‹¤ìš´ ê¸°ë°˜ ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "def predict_disease_from_card(symptom):\n",
    "    best_match = process.extractOne(symptom, df['Symptoms'].dropna())\n",
    "    if best_match and best_match[1] > 80:\n",
    "        disease_info = df.loc[df['Symptoms'] == best_match[0]]\n",
    "        disease_name = disease_info['Disease'].values[0]\n",
    "        symptoms = disease_info['Symptoms'].values[0]\n",
    "        description = disease_info['Description'].values[0]\n",
    "        return f\"ì§ˆë³‘: {disease_name}\\nì¦ìƒ: {symptoms}\\nì„¤ëª…: {description}\"\n",
    "    return \"í•´ë‹¹ ì¦ìƒì— ëŒ€í•œ ì§ˆë³‘ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\" ê³ ì–‘ì´ ì§ˆë³‘ ììœ í˜• AI ì±—ë´‡ \"):\n",
    "            with gr.Column():\n",
    "                gr.HTML(\"\"\"\n",
    "                    <div style=\"text-align: center; font-size: 24px; color: #7193BD; font-weight: bold; margin-bottom: 15px;\">ê³ ì–‘ì´ ì§ˆë³‘ ìê°€ì§„ë‹¨ ì±—ë´‡</div>\n",
    "                \"\"\")\n",
    "                chatbot = gr.ChatInterface(\n",
    "                    fn=conversational_chat,\n",
    "                    examples=[\n",
    "                        \"ê³ ì–‘ì´ê°€ ì‹ì‚¬ ê±°ë¶€í•˜ê³  ì¹¨ì„ ë§ì´ í˜ë ¤ìš”.\",\n",
    "                        \"ê³ ì–‘ì´ê°€ ë³µë¶€ì— ì´ìƒì´ ìˆëŠ” ê²ƒ ê°™ì•„ìš”.\",\n",
    "                        \"ê³ ì–‘ì´ì˜ ì¦ìƒì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "                        \"ê³ ì–‘ì´ ë²”ë°±í˜ˆêµ¬ ê°ì†Œì¦ì˜ ì›ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "                        \"ê³ ì–‘ì´ì—ê²Œ í”í•œ ì§ˆë³‘ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "        with gr.TabItem(\" ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ - ì–‘ì‹ ê¸°ë°˜ \"):\n",
    "            gr.Markdown(\"## ğŸ¾ ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ ì„œë¹„ìŠ¤ - ì–‘ì‹ ê¸°ë°˜\")\n",
    "            gr.Markdown(\"### ê³ ì–‘ì´ì˜ ìƒíƒœë¥¼ ìì„¸íˆ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "            symptom_input = gr.Textbox(label=\"ì¦ìƒ ì…ë ¥\", placeholder=\"ì˜ˆ: ê³ ì–‘ì´ê°€ ë°¥ì„ ì•ˆ ë¨¹ê³  êµ¬í† ë¥¼ í•´ìš”.\")\n",
    "            age_input = gr.Number(label=\"ê³ ì–‘ì´ ë‚˜ì´ (ë…„)\", value=3)\n",
    "            duration_input = gr.Number(label=\"ì¦ìƒ ì§€ì† ê¸°ê°„ (ì¼)\", value=1)\n",
    "\n",
    "            output_box = gr.Textbox(label=\"ì§ˆë³‘ ì§„ë‹¨ ê²°ê³¼\", lines=10, interactive=False)\n",
    "            submit_button = gr.Button(\"ì§„ë‹¨ ìš”ì²­\")\n",
    "            submit_button.click(fn=predict_from_form, inputs=[symptom_input, age_input, duration_input], outputs=output_box)\n",
    "\n",
    "        with gr.TabItem(\" ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ - ë“œë¡­ë‹¤ìš´ ê¸°ë°˜ \"):\n",
    "            gr.Markdown(\"## ğŸ¾ ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ ì„œë¹„ìŠ¤ - ë“œë¡­ë‹¤ìš´ ê¸°ë°˜\")\n",
    "            gr.Markdown(\"### ê³ ì–‘ì´ì˜ ì¦ìƒ ì¤‘ í•˜ë‚˜ë¥¼ ë“œë¡­ë‹¤ìš´ì—ì„œ ì„ íƒí•˜ì„¸ìš”.\")\n",
    "            \n",
    "            # ì¦ìƒ ëª©ë¡ì„ ë“œë¡­ë‹¤ìš´ì— ì¶”ê°€\n",
    "            symptoms = df['Symptoms'].dropna().unique().tolist()\n",
    "            \n",
    "            symptom_dropdown = gr.Dropdown(choices=symptoms, label=\"ì¦ìƒ ì„ íƒ\", interactive=True)\n",
    "            output_box = gr.Textbox(label=\"ì§ˆë³‘ ì§„ë‹¨ ê²°ê³¼\", lines=10, interactive=False)\n",
    "\n",
    "            submit_button = gr.Button(\"ì§„ë‹¨ ìš”ì²­\")\n",
    "            submit_button.click(fn=predict_disease_from_card, inputs=symptom_dropdown, outputs=output_box)\n",
    "\n",
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\", share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
