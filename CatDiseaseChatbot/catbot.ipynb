{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê³ ì–‘ì´ ì§ˆë³‘ ìê°€ì§„ë‹¨ ì±—ë´‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from fuzzywuzzy import process\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³ ì–‘ì´ ì§ˆë³‘ ê´€ë ¨ ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” CSV íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "df = pd.read_csv(\"./data/cat_diseases.csv\", encoding='CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symptomsì™€ Description ì»¬ëŸ¼ì„ í•©ì³ì„œ ìƒˆë¡œìš´ inputs ì»¬ëŸ¼ ìƒì„±\n",
    "# ì´ ì»¬ëŸ¼ì€ ê²€ìƒ‰ì— ì‚¬ìš©ë  í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "df['inputs'] = df['Symptoms'].fillna('') + \" \" + df['Description'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¼ì • í¬ê¸°ë¡œ ë¶„í• í•©ë‹ˆë‹¤ (chunk_size: 500, chunk_overlap: 200)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(\"\\n\".join(df['inputs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-28\\AppData\\Local\\Temp\\ipykernel_9616\\1657823153.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\HJY310\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìœ„í•´ HuggingFaceì˜ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶„í• ëœ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-28\\AppData\\Local\\Temp\\ipykernel_9616\\1599780073.py:3: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"gemma2\", temperature=0.0)\n"
     ]
    }
   ],
   "source": [
    "# ChatOllama ëª¨ë¸ ì´ˆê¸°í™” (ì˜¨ë„ê°’ temperatureëŠ” 0.0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¼ê´€ëœ ì‘ë‹µ ìƒì„±)\n",
    "try:\n",
    "    llm = ChatOllama(model=\"gemma2\", temperature=0.0)\n",
    "except Exception as e:\n",
    "    print(f\"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ˆë¬¸ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "question_generator_template = \"\"\"\n",
    "ì´ì „ ëŒ€í™” ë‚´ì—­ê³¼ ìƒˆë¡œìš´ ì‚¬ìš©ì ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ê²€ìƒ‰ì„ ìœ„í•œ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¿”ì„œ ìƒì„±í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì´ì „ ëŒ€í™” ë‚´ì—­:\n",
    "{chat_history}\n",
    "\n",
    "ìƒˆë¡œìš´ ì‚¬ìš©ì ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ë…ë¦½ì ì¸ ì§ˆë¬¸:\n",
    "\"\"\"\n",
    "QUESTION_GENERATOR_PROMPT = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=question_generator_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (í•œêµ­ì–´ì— ìµœì í™”)\n",
    "combine_documents_template = \"\"\"\n",
    "ë‹¹ì‹ ì€ ê²½í—˜ ë§ì€ ê³ ì–‘ì´ ìˆ˜ì˜ì‚¬ì…ë‹ˆë‹¤. **ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ì—¬, ì–´ìƒ‰í•¨ì´ ì—†ë„ë¡ ë§¤ìš° ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì—¬ ì •ì¤‘í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.** ì™¸êµ­ì–´ë‚˜ ì–´ìƒ‰í•œ í•œêµ­ì–´ í‘œí˜„ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì‹­ì‹œì˜¤.\n",
    "\n",
    "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "1.  ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€ì„ **í•œêµ­ì–´ë¡œ** ì œê³µí•©ë‹ˆë‹¤.\n",
    "2.  í•„ìš”í•œ ê²½ìš°, ì¶”ê°€ì ì¸ ì§ˆë¬¸ì„ í†µí•´ ìƒí™©ì„ ëª…í™•íˆ íŒŒì•…í•˜ë ¤ê³  ë…¸ë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¦ìƒì˜ ê¸°ê°„, ì‹¬ê°ì„±, ë‹¤ë¥¸ ë™ë°˜ ì¦ìƒ ë“±ì„ **í•œêµ­ì–´ë¡œ, ë¶€ë“œëŸ½ê²Œ** ë¬¼ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜ˆ: \"í˜¹ì‹œ ì–¸ì œë¶€í„° ê·¸ëŸ¬ì…¨ë‚˜ìš”?\", \"ë‹¤ë¥¸ ë¶ˆí¸í•œ ì ì€ ì—†ìœ¼ì‹ ê°€ìš”?\")\n",
    "3.  ê°€ëŠ¥í•œ ì›ì¸ ì§ˆë³‘ì„ ì–¸ê¸‰í•˜ê³ , ê° ì§ˆë³‘ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…ì„ **í•œêµ­ì–´ë¡œ, ì´í•´í•˜ê¸° ì‰½ê²Œ** ì œê³µí•©ë‹ˆë‹¤.\n",
    "4.  ì§‘ì—ì„œ í•  ìˆ˜ ìˆëŠ” ì¡°ì¹˜ì™€ ë™ë¬¼ë³‘ì› ë°©ë¬¸ì´ í•„ìš”í•œ ê²½ìš°ë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ **í•œêµ­ì–´ë¡œ, ì¹œì ˆí•˜ê²Œ** ì•ˆë‚´í•©ë‹ˆë‹¤.\n",
    "5.  ì ˆëŒ€ ì§„ë‹¨ì´ë‚˜ ì²˜ë°©ì„ ë‚´ë¦¬ì§€ ì•Šê³ , ë°˜ë“œì‹œ ë™ë¬¼ë³‘ì›ì— ë°©ë¬¸í•˜ì—¬ ì •í™•í•œ ì§„ë£Œë¥¼ ë°›ì„ ê²ƒì„ **í•œêµ­ì–´ë¡œ, ì •ì¤‘í•˜ê²Œ** ê¶Œì¥í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "6.  ì‚¬ìš©ìê°€ ê±±ì •í•˜ê±°ë‚˜ ë¶ˆì•ˆí•œ ê°ì •ì„ ëŠë‚„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, **ìœ„ë¡œ**ì™€ **ì•ˆì‹¬**ì„ ì£¼ëŠ” ë§ì„ í¬í•¨í•˜ì—¬ ì‚¬ìš©ìê°€ ë” í¸ì•ˆí•˜ê²Œ ëŠë¼ë„ë¡ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê²€ìƒ‰ëœ ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ì‚¬ìš©ì ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "ë‹µë³€:\n",
    "\"\"\"\n",
    "COMBINE_DOCUMENTS_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=combine_documents_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-28\\AppData\\Local\\Temp\\ipykernel_9616\\4189892614.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  question_generator = LLMChain(llm=llm, prompt=QUESTION_GENERATOR_PROMPT)\n",
      "C:\\Users\\human-28\\AppData\\Local\\Temp\\ipykernel_9616\\4189892614.py:4: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
      "  doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=COMBINE_DOCUMENTS_PROMPT)\n",
      "C:\\Users\\human-28\\AppData\\Local\\Temp\\ipykernel_9616\\4189892614.py:6: LangChainDeprecationWarning: The class `ConversationalRetrievalChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~create_history_aware_retriever together with create_retrieval_chain (see example in docstring)` instead.\n",
      "  qa = ConversationalRetrievalChain(\n"
     ]
    }
   ],
   "source": [
    "# Conversational Retrieval Chain ìƒì„±\n",
    "# ì´ì „ ëŒ€í™” ë‚´ì—­ê³¼ ì‚¬ìš©ì ì§ˆë¬¸ì„ ê²°í•©í•´ ë…ë¦½ì ì¸ ì§ˆë¬¸ ìƒì„± í›„ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "question_generator = LLMChain(llm=llm, prompt=QUESTION_GENERATOR_PROMPT)\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=COMBINE_DOCUMENTS_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain(\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 1}),  # ê²€ìƒ‰ëœ ìƒìœ„ 1ê°œì˜ ë¬¸ì„œ ì‚¬ìš©\n",
    "    combine_docs_chain=doc_chain,\n",
    "    question_generator=question_generator,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "chat_history = []  # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜ (íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜)\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.replace(\" \", \"\")\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³ ì–‘ì´ ì§ˆë³‘ ì •ë³´ ê²€ìƒ‰ í•¨ìˆ˜\n",
    "def search_disease_info(message):\n",
    "    return \"ì§ˆë³‘ ì •ë³´ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³ ì–‘ì´ ì§ˆë³‘ì— ëŒ€í•œ ì •ë³´ ì œê³µí•˜ëŠ” ì±„íŒ… í•¨ìˆ˜\n",
    "def chat(message, history):\n",
    "    cleaned_message = clean_text(message)\n",
    "    disease_info = search_disease_info(message)\n",
    "    return disease_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversational Chat í•¨ìˆ˜ êµ¬í˜„ (chat_history ê´€ë¦¬)\n",
    "def conversational_chat(message, history):\n",
    "    # QA ìˆ˜í–‰\n",
    "    result = qa({\"question\": message, \"chat_history\": history})\n",
    "    \n",
    "    # Gradioê°€ historyë¥¼ ê´€ë¦¬í•˜ë¯€ë¡œ historyë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ\n",
    "    response = result[\"answer\"]\n",
    "    return response  # ì‘ë‹µë§Œ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¦ìƒ,ë‚˜ì´ ì…ë ¥í•˜ëŠ” ê°„ë‹¨ ì§„ë‹¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„¸ ë²ˆì§¸ íƒ­ìš© í•¨ìˆ˜\n",
    "custom_prompt = \"\"\"\n",
    "ë‹¹ì‹ ì€ ê³ ì–‘ì´ ì§ˆë³‘ì— ëŒ€í•´ ì „ë¬¸ì ì¸ ìˆ˜ì˜ì‚¬ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "ì‚¬ìš©ìê°€ ê±±ì •í•˜ê±°ë‚˜ ë¶ˆì•ˆí•œ ê°ì •ì„ ëŠë‚„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, **ìœ„ë¡œ**ì™€ **ì•ˆì‹¬**ì„ ì£¼ëŠ” ë§ì„ í¬í•¨í•˜ì—¬ ì‚¬ìš©ìê°€ ë” í¸ì•ˆí•˜ê²Œ ëŠë¼ë„ë¡ í•©ë‹ˆë‹¤.\n",
    "ì•„ë˜ ì¦ìƒì— ê¸°ë°˜í•˜ì—¬ ê°€ëŠ¥í•œ ì§ˆë³‘ê³¼ ê´€ë ¨ ì •ë³´ë¥¼ ì‘ì„±í•˜ì„¸ìš”:\n",
    "\n",
    "ì¦ìƒ: {symptom}\n",
    "\n",
    "ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\"\"\"\n",
    "PROMPT_TEMPLATE = PromptTemplate(input_variables=[\"symptom\"], template=custom_prompt)\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš©ìê°€ ì–‘ì‹ì— ì…ë ¥í•œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜\n",
    "def predict_from_form(symptom, age, duration):\n",
    "    prompt = f\"\"\"\n",
    "    ê³ ì–‘ì´ ë‚˜ì´: {age}\n",
    "    ì¦ìƒ ë°œìƒ ê¸°ê°„: {duration}ì¼\n",
    "    ì¦ìƒ: {symptom}\n",
    "    \n",
    "    ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ëŠ¥í•œ ì§ˆë³‘ê³¼ ëŒ€ì²˜ ë°©ë²•ì„ ì‘ì„±í•˜ì„¸ìš”.\n",
    "    \"\"\"\n",
    "    response = chain.run(symptom=prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csvë°ì´í„°ê¸°ë°˜ ë°”ë¡œ ì‘ë‹µ ê°€ëŠ¥í•œ ì§„ì§œ ê°„ë‹¨ ì§„ë‹¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë“œë¡­ë‹¤ìš´ì—ì„œ ì„ íƒí•œ ì¦ìƒì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë³‘ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜\n",
    "def predict_disease_from_card(symptom):\n",
    "    # Fuzzy Matchingì„ ì‚¬ìš©í•´ ì¦ìƒê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ ì°¾ìŒ\n",
    "    best_match = process.extractOne(symptom, df['Symptoms'].dropna())\n",
    "    if best_match and best_match[1] > 80:  # ìœ ì‚¬ë„ê°€ 80% ì´ìƒì¸ ê²½ìš°\n",
    "        disease_info = df.loc[df['Symptoms'] == best_match[0]]\n",
    "        disease_name = disease_info['Disease'].values[0]\n",
    "        symptoms = disease_info['Symptoms'].values[0]\n",
    "        description = disease_info['Description'].values[0]\n",
    "        return f\"ì§ˆë³‘: {disease_name}\\nì¦ìƒ: {symptoms}\\nì„¤ëª…: {description}\"\n",
    "    return \"í•´ë‹¹ ì¦ìƒì— ëŒ€í•œ ì§ˆë³‘ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradio_Taps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\HJY310\\.venv\\lib\\site-packages\\gradio\\components\\chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    with gr.Tabs():\n",
    "        \n",
    "    # 1. csvê¸°ë°˜ ë“œë¡­ë‹¤ìš´ ê°„ë‹¨í˜•ì‹\n",
    "        with gr.TabItem(\" ğŸš‘ ê³ ì–‘ì´ ì§ˆë³‘ ê°„ë‹¨ ì˜ˆì¸¡ ğŸš‘ \"):\n",
    "            gr.Markdown(\"\"\"\n",
    "                    <div style=\"text-align: center; font-size: 24px; font-weight: bold; color: #7193BD; margin:30px;\"> \n",
    "                    ğŸ¾ ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ ì„œë¹„ìŠ¤ - ë“œë¡­ë‹¤ìš´ğŸ¾ </div>\n",
    "                \"\"\")\n",
    "            gr.Markdown(\"\"\"\n",
    "                    <div style=\"text-align: right; color: #868e96; font-weight: bold; margin-bottom: 30px;\">\n",
    "                    ê³ ì–‘ì´ì˜ ì¦ìƒ ì¤‘ ë¹„ìŠ·í•œ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n",
    "                    </div>\n",
    "                \"\"\")\n",
    "            \n",
    "            # ì¦ìƒ ëª©ë¡ì„ ë“œë¡­ë‹¤ìš´ì— ì¶”ê°€\n",
    "            symptoms = df['Symptoms'].dropna().unique().tolist()\n",
    "            \n",
    "            symptom_dropdown = gr.Dropdown(choices=symptoms, label=\"ì¦ìƒ ì„ íƒ\", interactive=True)\n",
    "            output_box = gr.Textbox(label=\"ì§ˆë³‘ ì§„ë‹¨ ê²°ê³¼\", lines=10, interactive=False)\n",
    "\n",
    "            submit_button = gr.Button(\"ì§„ë‹¨ ìš”ì²­\")\n",
    "            submit_button.click(fn=predict_disease_from_card, inputs=symptom_dropdown, outputs=output_box)\n",
    "\n",
    "    # 2. ì¦ìƒ/ë‚˜ì´/ì¦ìƒê¸°ê°„ ì–‘ì‹ì„¤ì •ì— ë”°ë¥¸ í˜•ì‹\n",
    "        with gr.TabItem(\" ğŸ” ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ ğŸ” \"):\n",
    "            gr.Markdown(\"\"\"\n",
    "                    <div style=\"text-align: center; font-size: 24px; font-weight: bold; color: #7193BD; margin: 30px;\"> \n",
    "                    ğŸ¾ ê³ ì–‘ì´ ì§ˆë³‘ ì˜ˆì¸¡ ì„œë¹„ìŠ¤ - ì–‘ì‹ ê¸°ë°˜ğŸ¾ </div>\n",
    "                \"\"\")\n",
    "            gr.Markdown(\"\"\"\n",
    "                    <div style=\"text-align: right; color: #868e96; font-weight: bold; margin-bottom: 30px;\">\n",
    "                    ê³ ì–‘ì´ì˜ ìƒíƒœë¥¼ ìì„¸íˆ ì…ë ¥í•˜ì„¸ìš”.\n",
    "                    </div>\n",
    "                \"\"\")\n",
    "\n",
    "            symptom_input = gr.Textbox(label=\"ì¦ìƒ ì…ë ¥\", placeholder=\"ì˜ˆ: ê³ ì–‘ì´ê°€ ë°¥ì„ ì•ˆ ë¨¹ê³  êµ¬í† ë¥¼ í•´ìš”.\")\n",
    "            age_input = gr.Number(label=\"ê³ ì–‘ì´ ë‚˜ì´ (ë…„)\", value=3)\n",
    "            duration_input = gr.Number(label=\"ì¦ìƒ ì§€ì† ê¸°ê°„ (ì¼)\", value=1)\n",
    "\n",
    "            output_box = gr.Textbox(label=\"ì§ˆë³‘ ì§„ë‹¨ ê²°ê³¼\", lines=10, interactive=False)\n",
    "            submit_button = gr.Button(\"ì§„ë‹¨ ìš”ì²­\")\n",
    "            submit_button.click(fn=predict_from_form, inputs=[symptom_input, age_input, duration_input], outputs=output_box)\n",
    "\n",
    "\n",
    "    # 3. ììœ í˜• ì±—ë´‡\n",
    "        with gr.TabItem(\" ğŸ¤– ê³ ì–‘ì´ ì§ˆë³‘ ììœ í˜• AI ì±—ë´‡ ğŸ¤– \"):\n",
    "            with gr.Column():\n",
    "                gr.HTML(\"\"\"\n",
    "                    <div style=\"text-align: center; font-size: 24px; color: #7193BD; font-weight: bold; margin-bottom: 15px;\">ğŸ˜ºê³ ì–‘ì´ ì§ˆë³‘ ìê°€ì§„ë‹¨ ì±—ë´‡ğŸ˜º</div>\n",
    "                \"\"\")\n",
    "                chatbot = gr.ChatInterface(\n",
    "                    fn=conversational_chat,\n",
    "                    examples=[\n",
    "                        \"ê³ ì–‘ì´ê°€ ì‹ì‚¬ ê±°ë¶€í•˜ê³  ì¹¨ì„ ë§ì´ í˜ë ¤ìš”.\",\n",
    "                        \"ê³ ì–‘ì´ê°€ ë³µë¶€ì— ì´ìƒì´ ìˆëŠ” ê²ƒ ê°™ì•„ìš”.\",\n",
    "                        \"ê³ ì–‘ì´ê°€ ì‹ì‚¬ë¥¼ ê±°ë¶€í•´ìš”.\",\n",
    "                        \"ê³ ì–‘ì´ê°€ ë¬´ê¸°ë ¥í•˜ê³  ê·¸ë£¨ë°ì„ ê³¼í•˜ê²Œí•´ìš”\",\n",
    "                        \"ê³ ì–‘ì´ì—ê²Œ í”í•œ ì§ˆë³‘ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "                        \"ê³ ì–‘ì´ì—ê²Œ ì•ˆì¢‹ì€ ì‹ë¬¼ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "                    ]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* Running on public URL: https://053984fdfee19ee67e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://053984fdfee19ee67e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\", share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
