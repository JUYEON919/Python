{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "고양이 질병 자가진단 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from fuzzywuzzy import process\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고양이 질병 관련 데이터를 포함하는 CSV 파일을 로드합니다.\n",
    "df = pd.read_csv(\"./data/cat_diseases.csv\", encoding='CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symptoms와 Description 컬럼을 합쳐서 새로운 inputs 컬럼 생성\n",
    "# 이 컬럼은 검색에 사용될 텍스트 데이터를 만듭니다.\n",
    "df['inputs'] = df['Symptoms'].fillna('') + \" \" + df['Description'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 데이터를 일정 크기로 분할합니다 (chunk_size: 500, chunk_overlap: 200)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(\"\\n\".join(df['inputs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-28\\AppData\\Local\\Temp\\ipykernel_9616\\1657823153.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\HJY310\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 임베딩을 위해 HuggingFace의 사전 학습된 모델을 사용합니다.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할된 텍스트를 기반으로 FAISS 벡터 데이터베이스 생성\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-28\\AppData\\Local\\Temp\\ipykernel_9616\\1599780073.py:3: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"gemma2\", temperature=0.0)\n"
     ]
    }
   ],
   "source": [
    "# ChatOllama 모델 초기화 (온도값 temperature는 0.0으로 설정하여 일관된 응답 생성)\n",
    "try:\n",
    "    llm = ChatOllama(model=\"gemma2\", temperature=0.0)\n",
    "except Exception as e:\n",
    "    print(f\"LLM 초기화 오류: {e}\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 생성용 프롬프트 템플릿 정의\n",
    "question_generator_template = \"\"\"\n",
    "이전 대화 내역과 새로운 사용자 질문이 주어졌을 때, 검색을 위한 독립적인 질문으로 바꿔서 생성해주세요.\n",
    "\n",
    "이전 대화 내역:\n",
    "{chat_history}\n",
    "\n",
    "새로운 사용자 질문:\n",
    "{question}\n",
    "\n",
    "독립적인 질문:\n",
    "\"\"\"\n",
    "QUESTION_GENERATOR_PROMPT = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=question_generator_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색된 문서를 기반으로 답변 생성용 프롬프트 템플릿 정의 (한국어에 최적화)\n",
    "combine_documents_template = \"\"\"\n",
    "당신은 경험 많은 고양이 수의사입니다. **모든 답변은 한국어만 사용하여, 어색함이 없도록 매우 자연스럽게 작성해야 합니다. 존댓말을 사용하여 정중하게 답변해야 합니다.** 외국어나 어색한 한국어 표현을 절대 사용하지 않도록 주의하십시오.\n",
    "\n",
    "사용자의 질문에 대해 다음과 같은 방식으로 답변해야 합니다.\n",
    "\n",
    "1.  질문에 대한 직접적인 답변을 **한국어로** 제공합니다.\n",
    "2.  필요한 경우, 추가적인 질문을 통해 상황을 명확히 파악하려고 노력해야 합니다. 예를 들어, 증상의 기간, 심각성, 다른 동반 증상 등을 **한국어로, 부드럽게** 물어볼 수 있습니다. (예: \"혹시 언제부터 그러셨나요?\", \"다른 불편한 점은 없으신가요?\")\n",
    "3.  가능한 원인 질병을 언급하고, 각 질병에 대한 간략한 설명을 **한국어로, 이해하기 쉽게** 제공합니다.\n",
    "4.  집에서 할 수 있는 조치와 동물병원 방문이 필요한 경우를 명확하게 구분하여 **한국어로, 친절하게** 안내합니다.\n",
    "5.  절대 진단이나 처방을 내리지 않고, 반드시 동물병원에 방문하여 정확한 진료를 받을 것을 **한국어로, 정중하게** 권장해야 합니다.\n",
    "6.  사용자가 걱정하거나 불안한 감정을 느낄 수 있기 때문에, **위로**와 **안심**을 주는 말을 포함하여 사용자가 더 편안하게 느끼도록 합니다.\n",
    "\n",
    "검색된 문서:\n",
    "{context}\n",
    "\n",
    "사용자 질문:\n",
    "{question}\n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    "COMBINE_DOCUMENTS_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=combine_documents_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-28\\AppData\\Local\\Temp\\ipykernel_9616\\4189892614.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  question_generator = LLMChain(llm=llm, prompt=QUESTION_GENERATOR_PROMPT)\n",
      "C:\\Users\\human-28\\AppData\\Local\\Temp\\ipykernel_9616\\4189892614.py:4: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
      "  doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=COMBINE_DOCUMENTS_PROMPT)\n",
      "C:\\Users\\human-28\\AppData\\Local\\Temp\\ipykernel_9616\\4189892614.py:6: LangChainDeprecationWarning: The class `ConversationalRetrievalChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~create_history_aware_retriever together with create_retrieval_chain (see example in docstring)` instead.\n",
      "  qa = ConversationalRetrievalChain(\n"
     ]
    }
   ],
   "source": [
    "# Conversational Retrieval Chain 생성\n",
    "# 이전 대화 내역과 사용자 질문을 결합해 독립적인 질문 생성 후 검색된 문서를 기반으로 답변을 생성합니다.\n",
    "question_generator = LLMChain(llm=llm, prompt=QUESTION_GENERATOR_PROMPT)\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=COMBINE_DOCUMENTS_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain(\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 1}),  # 검색된 상위 1개의 문서 사용\n",
    "    combine_docs_chain=doc_chain,\n",
    "    question_generator=question_generator,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "chat_history = []  # 대화 기록 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력된 텍스트를 정리하는 함수 (특수문자 제거 및 소문자 변환)\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.replace(\" \", \"\")\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고양이 질병 정보 검색 함수\n",
    "def search_disease_info(message):\n",
    "    return \"질병 정보를 찾고 있습니다...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고양이 질병에 대한 정보 제공하는 채팅 함수\n",
    "def chat(message, history):\n",
    "    cleaned_message = clean_text(message)\n",
    "    disease_info = search_disease_info(message)\n",
    "    return disease_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversational Chat 함수 구현 (chat_history 관리)\n",
    "def conversational_chat(message, history):\n",
    "    # QA 수행\n",
    "    result = qa({\"question\": message, \"chat_history\": history})\n",
    "    \n",
    "    # Gradio가 history를 관리하므로 history를 직접 업데이트하지 않음\n",
    "    response = result[\"answer\"]\n",
    "    return response  # 응답만 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "증상,나이 입력하는 간단 진단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 번째 탭용 함수\n",
    "custom_prompt = \"\"\"\n",
    "당신은 고양이 질병에 대해 전문적인 수의사 역할을 합니다.\n",
    "사용자가 걱정하거나 불안한 감정을 느낄 수 있기 때문에, **위로**와 **안심**을 주는 말을 포함하여 사용자가 더 편안하게 느끼도록 합니다.\n",
    "아래 증상에 기반하여 가능한 질병과 관련 정보를 작성하세요:\n",
    "\n",
    "증상: {symptom}\n",
    "\n",
    "답변은 반드시 한국어로 작성하세요.\n",
    "\"\"\"\n",
    "PROMPT_TEMPLATE = PromptTemplate(input_variables=[\"symptom\"], template=custom_prompt)\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자가 양식에 입력한 정보를 기반으로 예측하는 함수\n",
    "def predict_from_form(symptom, age, duration):\n",
    "    prompt = f\"\"\"\n",
    "    고양이 나이: {age}\n",
    "    증상 발생 기간: {duration}일\n",
    "    증상: {symptom}\n",
    "    \n",
    "    위 정보를 기반으로 가능한 질병과 대처 방법을 작성하세요.\n",
    "    \"\"\"\n",
    "    response = chain.run(symptom=prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv데이터기반 바로 응답 가능한 진짜 간단 진단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드롭다운에서 선택한 증상을 기반으로 질병을 예측하는 함수\n",
    "def predict_disease_from_card(symptom):\n",
    "    # Fuzzy Matching을 사용해 증상과 가장 유사한 데이터를 찾음\n",
    "    best_match = process.extractOne(symptom, df['Symptoms'].dropna())\n",
    "    if best_match and best_match[1] > 80:  # 유사도가 80% 이상인 경우\n",
    "        disease_info = df.loc[df['Symptoms'] == best_match[0]]\n",
    "        disease_name = disease_info['Disease'].values[0]\n",
    "        symptoms = disease_info['Symptoms'].values[0]\n",
    "        description = disease_info['Description'].values[0]\n",
    "        return f\"질병: {disease_name}\\n증상: {symptoms}\\n설명: {description}\"\n",
    "    return \"해당 증상에 대한 질병 정보를 찾을 수 없습니다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradio_Taps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\HJY310\\.venv\\lib\\site-packages\\gradio\\components\\chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio 인터페이스 설정\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    with gr.Tabs():\n",
    "        \n",
    "    # 1. csv기반 드롭다운 간단형식\n",
    "        with gr.TabItem(\" 🚑 고양이 질병 간단 예측 🚑 \"):\n",
    "            gr.Markdown(\"\"\"\n",
    "                    <div style=\"text-align: center; font-size: 24px; font-weight: bold; color: #7193BD; margin:30px;\"> \n",
    "                    🐾 고양이 질병 예측 서비스 - 드롭다운🐾 </div>\n",
    "                \"\"\")\n",
    "            gr.Markdown(\"\"\"\n",
    "                    <div style=\"text-align: right; color: #868e96; font-weight: bold; margin-bottom: 30px;\">\n",
    "                    고양이의 증상 중 비슷한 하나를 선택하세요.\n",
    "                    </div>\n",
    "                \"\"\")\n",
    "            \n",
    "            # 증상 목록을 드롭다운에 추가\n",
    "            symptoms = df['Symptoms'].dropna().unique().tolist()\n",
    "            \n",
    "            symptom_dropdown = gr.Dropdown(choices=symptoms, label=\"증상 선택\", interactive=True)\n",
    "            output_box = gr.Textbox(label=\"질병 진단 결과\", lines=10, interactive=False)\n",
    "\n",
    "            submit_button = gr.Button(\"진단 요청\")\n",
    "            submit_button.click(fn=predict_disease_from_card, inputs=symptom_dropdown, outputs=output_box)\n",
    "\n",
    "    # 2. 증상/나이/증상기간 양식설정에 따른 형식\n",
    "        with gr.TabItem(\" 🔍 고양이 질병 예측 🔍 \"):\n",
    "            gr.Markdown(\"\"\"\n",
    "                    <div style=\"text-align: center; font-size: 24px; font-weight: bold; color: #7193BD; margin: 30px;\"> \n",
    "                    🐾 고양이 질병 예측 서비스 - 양식 기반🐾 </div>\n",
    "                \"\"\")\n",
    "            gr.Markdown(\"\"\"\n",
    "                    <div style=\"text-align: right; color: #868e96; font-weight: bold; margin-bottom: 30px;\">\n",
    "                    고양이의 상태를 자세히 입력하세요.\n",
    "                    </div>\n",
    "                \"\"\")\n",
    "\n",
    "            symptom_input = gr.Textbox(label=\"증상 입력\", placeholder=\"예: 고양이가 밥을 안 먹고 구토를 해요.\")\n",
    "            age_input = gr.Number(label=\"고양이 나이 (년)\", value=3)\n",
    "            duration_input = gr.Number(label=\"증상 지속 기간 (일)\", value=1)\n",
    "\n",
    "            output_box = gr.Textbox(label=\"질병 진단 결과\", lines=10, interactive=False)\n",
    "            submit_button = gr.Button(\"진단 요청\")\n",
    "            submit_button.click(fn=predict_from_form, inputs=[symptom_input, age_input, duration_input], outputs=output_box)\n",
    "\n",
    "\n",
    "    # 3. 자유형 챗봇\n",
    "        with gr.TabItem(\" 🤖 고양이 질병 자유형 AI 챗봇 🤖 \"):\n",
    "            with gr.Column():\n",
    "                gr.HTML(\"\"\"\n",
    "                    <div style=\"text-align: center; font-size: 24px; color: #7193BD; font-weight: bold; margin-bottom: 15px;\">😺고양이 질병 자가진단 챗봇😺</div>\n",
    "                \"\"\")\n",
    "                chatbot = gr.ChatInterface(\n",
    "                    fn=conversational_chat,\n",
    "                    examples=[\n",
    "                        \"고양이가 식사 거부하고 침을 많이 흘려요.\",\n",
    "                        \"고양이가 복부에 이상이 있는 것 같아요.\",\n",
    "                        \"고양이가 식사를 거부해요.\",\n",
    "                        \"고양이가 무기력하고 그루밍을 과하게해요\",\n",
    "                        \"고양이에게 흔한 질병은 무엇인가요?\",\n",
    "                        \"고양이에게 안좋은 식물은 무엇인가요?\"\n",
    "                    ]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* Running on public URL: https://053984fdfee19ee67e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://053984fdfee19ee67e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\", share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
